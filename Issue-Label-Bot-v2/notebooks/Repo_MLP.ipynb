{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "from repo_config import RepoConfig\n",
    "from mlp import MLPWrapper\n",
    "import dill as dpickle\n",
    "import os\n",
    "import yaml\n",
    "from google.cloud import storage\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from passlib.apps import custom_app_context as pwd_context\n",
    "from collections import Counter\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "class RepoMLP(object):\n",
    "\n",
    "    def __init__(self, yaml_path=None, owner=None, repo=None, min_freq=25):\n",
    "        if not yaml_path:\n",
    "            if 'YAML_PATH' in os.environ:\n",
    "                logging.info('yaml_path not supplied; check environment variable')\n",
    "                yaml_path = os.getenv('YAML_PATH')\n",
    "            else:\n",
    "                logging.info('yaml_path not supplied; using the default')\n",
    "                yaml_path = 'issue_label_bot.yaml'\n",
    "        self.yaml_path = yaml_path\n",
    "        self.min_freq = min_freq # for filtering labels\n",
    "        self.clf = None\n",
    "        self.all_labels = None\n",
    "        self.probability_thresholds = None\n",
    "        self.load_yaml(owner, repo)\n",
    "\n",
    "    def load_yaml(self, owner, repo):\n",
    "        config = RepoConfig(self.yaml_path, owner, repo)\n",
    "        self.repo_owner = config.repo_owner\n",
    "        self.repo_name = config.repo_name\n",
    "\n",
    "        self.model_bucket_name = config.model_bucket_name\n",
    "        self.model_file = config.model_local_path\n",
    "        self.model_dest = config.model_gcs_path\n",
    "\n",
    "        self.labels_file = config.labels_local_path\n",
    "        self.labels_dest = config.labels_gcs_path\n",
    "\n",
    "        self.embeddings_bucket_name = config.embeddings_bucket_name\n",
    "        self.embeddings_file = config.embeddings_local_path\n",
    "        self.embeddings_dest = config.embeddings_gcs_path\n",
    "\n",
    "    def download_embeddings_from_gcs(self):\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(self.embeddings_bucket_name)\n",
    "        blob = bucket.get_blob(self.embeddings_dest)\n",
    "        with open(self.embeddings_file, 'wb') as f:\n",
    "            blob.download_to_file(f)\n",
    "\n",
    "    def load_training_data(self):\n",
    "        self.download_embeddings_from_gcs()\n",
    "        with open(self.embeddings_file, 'rb') as f:\n",
    "            data = dpickle.load(f)\n",
    "\n",
    "        # filter labels\n",
    "        c = Counter()\n",
    "        for lbls in data['labels']:\n",
    "            c.update(lbls)\n",
    "        self.all_labels = [x for x in c if c[x] >= self.min_freq]\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        for emb, lbls in zip(data['features'], data['labels']):\n",
    "            mask = [self.all_labels.index(x) for x in lbls if c[x] >= self.min_freq]\n",
    "            if mask == []:\n",
    "                continue\n",
    "            zer = np.zeros(len(self.all_labels))\n",
    "            zer[mask] = 1\n",
    "            y.append(zer)\n",
    "            X.append(emb)\n",
    "        return X, y\n",
    "\n",
    "    def train(self):\n",
    "        X, y = self.load_training_data()\n",
    "        self.clf = MLPWrapper()\n",
    "        # get probability thresholds before `fit` because it overwrites classifier\n",
    "        self.clf.find_probability_thresholds(X, y)\n",
    "        self.probability_thresholds = self.clf.probability_thresholds\n",
    "        # train model using the whole data\n",
    "        self.clf.fit(X, y)\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        self.clf.save_model(model_file=self.model_file)\n",
    "        # dump label columns for prediction\n",
    "        label_dict = {'labels': self.all_labels, 'probability_thresholds': self.probability_thresholds}\n",
    "        with open(self.labels_file, 'wb') as f:\n",
    "            dpickle.dump(label_dict, f)\n",
    "\n",
    "        self.upload_model_to_gcs()\n",
    "\n",
    "    def upload_model_to_gcs(self):\n",
    "        # upload model\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(self.model_bucket_name)\n",
    "        blob = bucket.blob(self.model_dest)\n",
    "        blob.upload_from_filename(self.model_file)\n",
    "\n",
    "        # upload label columns\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(self.model_bucket_name)\n",
    "        blob = bucket.blob(self.labels_dest)\n",
    "        blob.upload_from_filename(self.labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run locally to test the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RepoMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry point using fairing\n",
    "Kubeflow [Fairing](https://www.kubeflow.org/docs/fairing/) is a Python package that makes training and deploying machine learning models on Kubeflow easier.\n",
    "\n",
    "Here, we use the preprocessor in Kubeflow Fairing to convert a notebook to be a Python script and create an entry point for that script. After preprocessing the notebook, we can call the command in the command line like the following to run\n",
    "```\n",
    "$ python Repo_MLP.py train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('Repo_MLP.py'), 'repo_config.py', 'mlp.py']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('RepoMLP')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['mlp.py', 'repo_config.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
