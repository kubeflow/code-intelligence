{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "This notebook illustrates the use of a utility, `InferenceWrapper.df_to_emb` that can be used to perform inference in bulk on large amounts of data.  A benchmark is provided that compares doing inference one at a time in a serial fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location of Model Artifacts\n",
    "\n",
    "### Google Cloud Storage\n",
    "\n",
    "- **model for inference** (965 MB): `https://storage.googleapis.com/issue_label_bot/model/lang_model/models_22zkdqlr/trained_model_22zkdqlr.pkl`\n",
    "\n",
    "\n",
    "- **encoder (for fine-tuning w/a classifier)** (965 MB): \n",
    "`https://storage.googleapis.com/issue_label_bot/model/lang_model/models_22zkdqlr/trained_model_encoder_22zkdqlr.pth`\n",
    "\n",
    "\n",
    "- **fastai.databunch** (27.1 GB):\n",
    "`https://storage.googleapis.com/issue_label_bot/model/lang_model/data_save.pkl`\n",
    "\n",
    "\n",
    "- **checkpointed model** (2.29 GB): \n",
    "`https://storage.googleapis.com/issue_label_bot/model/lang_model/models_22zkdqlr/best_22zkdqlr.pth`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Minimal Model For Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import InferenceWrapper, pass_through\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import Tensor, cat, device\n",
    "from torch.cuda import empty_cache\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from numpy import concatenate as cat\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# from fastai.torch_core import defaults\n",
    "# defaults.device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an `InferenceWrapper` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = InferenceWrapper(model_path='/ds/Issue-Embeddings/notebooks',\n",
    "                           model_file_name='trained_model_22zkdqlr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download a test dataset\n",
    "\n",
    "The test dataset has 2,000 GitHub Issues in the below format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repo</th>\n",
       "      <th>title</th>\n",
       "      <th>title_length</th>\n",
       "      <th>body</th>\n",
       "      <th>body_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/egingric/2016-Racing-Game/i...</td>\n",
       "      <td>egingric/2016-Racing-Game</td>\n",
       "      <td>Got stuck near shortcut</td>\n",
       "      <td>25</td>\n",
       "      <td>After being blown up by the barrel, I got stuc...</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/Microsoft/nodejstools/issue...</td>\n",
       "      <td>Microsoft/nodejstools</td>\n",
       "      <td>Guidance for unit test execution - How to prop...</td>\n",
       "      <td>95</td>\n",
       "      <td>What is the appropriate way to set NODE_ENV fo...</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/raphapari/dummy/issues/3</td>\n",
       "      <td>raphapari/dummy</td>\n",
       "      <td>Génération du catalogue</td>\n",
       "      <td>25</td>\n",
       "      <td>## User story xxxlnbrk - En tant que :  **gest...</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://github.com/egingric/2016-Racing-Game/i...   \n",
       "1  https://github.com/Microsoft/nodejstools/issue...   \n",
       "2        https://github.com/raphapari/dummy/issues/3   \n",
       "\n",
       "                        repo  \\\n",
       "0  egingric/2016-Racing-Game   \n",
       "1      Microsoft/nodejstools   \n",
       "2            raphapari/dummy   \n",
       "\n",
       "                                               title  title_length  \\\n",
       "0                            Got stuck near shortcut            25   \n",
       "1  Guidance for unit test execution - How to prop...            95   \n",
       "2                            Génération du catalogue            25   \n",
       "\n",
       "                                                body  body_length  \n",
       "0  After being blown up by the barrel, I got stuc...          314  \n",
       "1  What is the appropriate way to set NODE_ENV fo...          507  \n",
       "2  ## User story xxxlnbrk - En tant que :  **gest...          480  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = pd.read_csv(f'https://storage.googleapis.com/issue_label_bot/language_model_data/000000000000.csv.gz').head(8000)\n",
    "\n",
    "testdf.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Batch Inference\n",
    "\n",
    "Why Batch-Inference?  When there are a large number of issues for which you want to retrieve document embedddings, batch inference on a gpu (should be) significantly faster than on a cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Embeddings From Pre-Trained Language Model\n",
    "\n",
    "See help for `wrapper.df_to_emb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method df_to_emb in module inference:\n",
      "\n",
      "df_to_emb(dataframe:pandas.core.frame.DataFrame, bs=100) -> numpy.ndarray method of inference.InferenceWrapper instance\n",
      "    Retrieve document embeddings for a dataframe with the columns `title` and `body`.\n",
      "    Uses batching for effiecient computation, which is useful when you have many documents\n",
      "    to retrieve embeddings for. \n",
      "    \n",
      "    Paramaters\n",
      "    ----------\n",
      "    dataframe: pandas.DataFrame\n",
      "        Dataframe with columns `title` and `body`, which reprsent the Title and Body of a\n",
      "        GitHub Issue. \n",
      "    bs: int\n",
      "        batch size for doing inference.  Set this variable according to your available GPU memory.\n",
      "        The default is set to 200, which was stable on a Nvida-Tesla V-100.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    numpy.ndarray\n",
      "        An array with of shape (number of dataframe rows, 2400)\n",
      "        This numpy array represents the latent features of the GitHub issues.\n",
      "    \n",
      "    Example\n",
      "    -------\n",
      "    >>> import pandas as pd\n",
      "    >>> wrapper = InferenceWrapper(model_path='/path/to/model',\n",
      "                               model_file_name='model.pkl')\n",
      "    # load 200 sample GitHub issues\n",
      "    >>> testdf = pd.read_csv(f'https://bit.ly/2GDY5NY').head(200)\n",
      "    >>> embeddings = wrapper.df_to_emb(testdf)\n",
      "    \n",
      "    >>> embeddings.shape\n",
      "    (200, 2400)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wrapper.df_to_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking inference time on 8,000 Issues\n",
    "\n",
    "#### Below is when inference is done using batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cd443f57f7405584ce882b334c36d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Tokenizing and parsing text:', max=8000, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ba61b6a37e425aad97e5bfbbcd53f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Numericalizing text:', max=8000, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7ad9704b454b1fb5fe487a60d61952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Model inference:', max=1, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 9min 15s, sys: 13.1 s, total: 9min 28s\n",
      "Wall time: 9min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = wrapper.df_to_emb(testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is when inference is done one at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [14:57<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 50s, sys: 2min 55s, total: 15min 45s\n",
      "Wall time: 15min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prepare data\n",
    "test_data = [wrapper.process_dict(x)['text'] for x in testdf.to_dict(orient='rows')]\n",
    "\n",
    "emb_single = []\n",
    "for d in tqdm(test_data):\n",
    "    emb_single.append(wrapper.get_pooled_features(d).detach().cpu().numpy())\n",
    "    \n",
    "emb_single_combined = cat(emb_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "The bottleneck in the first method is the `Numericalizing` part.  When I tried to parallelize this with process based threading, the objects associated with the model could not be pickled and there were CUDA errors, so I left it alone.  There is a < **2x speedup** for inference by chunking the data into batches of similar length (to minimize padding) and passing that through the GPU.  This isn't that great but its due to the bottleneck.\n",
    "\n",
    "In order to get a further speed improvement we must utilize [pad_packed_sequence](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence).  We leave this a future exercise to optimize batching more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "This section tests that the embeddings retrieved from the one-at-a time approach are sufficently close to the embeddings from the batching approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(emb_single_combined, embeddings, atol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
