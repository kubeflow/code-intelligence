{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "from label_microservice.repo_config import RepoConfig\n",
    "from label_microservice.mlp import MLPWrapper\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import dill as dpickle\n",
    "import os\n",
    "import yaml\n",
    "from google.cloud import storage\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from passlib.apps import custom_app_context as pwd_context\n",
    "from collections import Counter\n",
    "from kfmd import metadata\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "class RepoMLP(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 yaml_path=None,\n",
    "                 owner=None,\n",
    "                 repo=None,\n",
    "                 workspace_name='train',\n",
    "                 min_freq=25,\n",
    "                 activation='relu',\n",
    "                 alpha=0.0001,\n",
    "                 early_stopping=True,\n",
    "                 epsilon=1e-08,\n",
    "                 hidden_layer_sizes=(100,),\n",
    "                 learning_rate='constant',\n",
    "                 learning_rate_init=0.001,\n",
    "                 max_iter=500,\n",
    "                 momentum=0.9,\n",
    "                 n_iter_no_change=5,\n",
    "                 random_state=1234,\n",
    "                 solver='adam',\n",
    "                 validation_fraction=0.1):\n",
    "        if not yaml_path:\n",
    "            if 'YAML_PATH' in os.environ:\n",
    "                logging.info('yaml_path not supplied; check environment variable')\n",
    "                yaml_path = os.getenv('YAML_PATH')\n",
    "            else:\n",
    "                logging.info('yaml_path not supplied; using the default')\n",
    "                yaml_path = 'issue_label_bot.yaml'\n",
    "        self.yaml_path = yaml_path\n",
    "        self.min_freq = min_freq # for filtering labels\n",
    "        self.mlp_wrapper = None\n",
    "        self.clf = MLPClassifier(activation=activation,\n",
    "                                 alpha=alpha,\n",
    "                                 early_stopping=early_stopping,\n",
    "                                 epsilon=epsilon,\n",
    "                                 hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                 learning_rate=learning_rate,\n",
    "                                 learning_rate_init=learning_rate_init,\n",
    "                                 max_iter=max_iter,\n",
    "                                 momentum=momentum,\n",
    "                                 n_iter_no_change=n_iter_no_change,\n",
    "                                 random_state=random_state,\n",
    "                                 solver=solver,\n",
    "                                 validation_fraction=validation_fraction)\n",
    "        self.all_labels = None\n",
    "        self.probability_thresholds = None\n",
    "        self.load_yaml(owner, repo)\n",
    "        self.exec = self.create_execution(workspace_name=workspace_name)\n",
    "\n",
    "    def load_yaml(self, owner, repo):\n",
    "        config = RepoConfig(self.yaml_path, owner, repo)\n",
    "        self.repo_owner = config.repo_owner\n",
    "        self.repo_name = config.repo_name\n",
    "\n",
    "        self.model_bucket_name = config.model_bucket_name\n",
    "        self.model_file = config.model_local_path\n",
    "        self.model_dest = config.model_gcs_path\n",
    "\n",
    "        self.labels_file = config.labels_local_path\n",
    "        self.labels_dest = config.labels_gcs_path\n",
    "\n",
    "        self.embeddings_bucket_name = config.embeddings_bucket_name\n",
    "        self.embeddings_file = config.embeddings_local_path\n",
    "        self.embeddings_dest = config.embeddings_gcs_path\n",
    "        \n",
    "        # TODO(chunhsiang): need to be able to train on multiple repos which\n",
    "        # should be defined in the yaml config\n",
    "        # for now, only train model on the repo installed\n",
    "        self.trained_repos = [f'{self.repo_owner}/{self.repo_name}']\n",
    "\n",
    "    def download_embeddings_from_gcs(self):\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(self.embeddings_bucket_name)\n",
    "        blob = bucket.get_blob(self.embeddings_dest)\n",
    "        with open(self.embeddings_file, 'wb') as f:\n",
    "            blob.download_to_file(f)\n",
    "\n",
    "    def load_training_data(self):\n",
    "        self.download_embeddings_from_gcs()\n",
    "        with open(self.embeddings_file, 'rb') as f:\n",
    "            data = dpickle.load(f)\n",
    "\n",
    "        # filter labels\n",
    "        c = Counter()\n",
    "        for lbls in data['labels']:\n",
    "            c.update(lbls)\n",
    "        self.all_labels = [x for x in c if c[x] >= self.min_freq]\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        for emb, lbls in zip(data['features'], data['labels']):\n",
    "            mask = [self.all_labels.index(x) for x in lbls if c[x] >= self.min_freq]\n",
    "            if mask == []:\n",
    "                continue\n",
    "            zer = np.zeros(len(self.all_labels))\n",
    "            zer[mask] = 1\n",
    "            y.append(zer)\n",
    "            X.append(emb)\n",
    "        return X, y\n",
    "\n",
    "    def train(self):\n",
    "        X, y = self.load_training_data()\n",
    "        self.mlp_wrapper = MLPWrapper(clf=self.clf)\n",
    "        # get probability thresholds before `fit` because it overwrites classifier\n",
    "        self.mlp_wrapper.find_probability_thresholds(X, y)\n",
    "        self.probability_thresholds = self.mlp_wrapper.probability_thresholds\n",
    "        # train model using the whole data\n",
    "        self.mlp_wrapper.fit(X, y)\n",
    "        self.save_model()\n",
    "\n",
    "        # store model artifacts using kubeflow metadata\n",
    "        model_name = ','.join(sorted(self.trained_repos))\n",
    "        model_uri = f'gs://{self.model_bucket_name}/{self.model_dest}'\n",
    "        # put all the repo names as the label keys\n",
    "        model_labels = {r:'' for r in self.trained_repos}\n",
    "        self.exec.log_output(metadata.Model(\n",
    "            name=model_name,\n",
    "            uri=model_uri,\n",
    "            labels=model_labels))\n",
    "\n",
    "    def save_model(self):\n",
    "        self.mlp_wrapper.save_model(model_file=self.model_file)\n",
    "        # dump label columns for prediction\n",
    "        label_dict = {'labels': self.all_labels, 'probability_thresholds': self.probability_thresholds}\n",
    "        with open(self.labels_file, 'wb') as f:\n",
    "            dpickle.dump(label_dict, f)\n",
    "\n",
    "        self.upload_model_to_gcs()\n",
    "\n",
    "    def upload_model_to_gcs(self):\n",
    "        # upload model\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(self.model_bucket_name)\n",
    "        blob = bucket.blob(self.model_dest)\n",
    "        blob.upload_from_filename(self.model_file)\n",
    "\n",
    "        # upload label columns\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(self.model_bucket_name)\n",
    "        blob = bucket.blob(self.labels_dest)\n",
    "        blob.upload_from_filename(self.labels_file)\n",
    "\n",
    "    def create_execution(self, workspace_name):\n",
    "        \"\"\"\n",
    "        Return a metatdata execution object in a workspace and\n",
    "        a run for logging.\n",
    "        Args:\n",
    "          workspace_name: workspace name, str\n",
    "        \"\"\"\n",
    "        workspace = metadata.Workspace(\n",
    "            # connect to metadata-service in namesapce kubeflow in k8s cluster.\n",
    "            backend_url_prefix='metadata-service.kubeflow:8080',\n",
    "            name=workspace_name,\n",
    "            description='workspace for model training artifacts and executions')\n",
    "        \n",
    "        run = metadata.Run(\n",
    "            workspace=workspace,\n",
    "            name='run-' + datetime.utcnow().isoformat('T'))\n",
    "\n",
    "        return metadata.Execution(\n",
    "            name = 'execution-' + datetime.utcnow().isoformat('T'),\n",
    "            workspace=workspace,\n",
    "            run=run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run locally to test the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RepoMLP(workspace_name='ws1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry point using fairing\n",
    "Kubeflow [Fairing](https://www.kubeflow.org/docs/fairing/) is a Python package that makes training and deploying machine learning models on Kubeflow easier.\n",
    "\n",
    "Here, we use the preprocessor in Kubeflow Fairing to convert a notebook to be a Python script and create an entry point for that script. After preprocessing the notebook, we can call the command in the command line like the following to run\n",
    "```\n",
    "$ python repo_mlp.py train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('repo_mlp.py'), 'repo_config.py', 'mlp.py']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ConvertNotebookPreprocessorWithFire('RepoMLP')\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "input_files = ['mlp.py', 'repo_config.py']\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in input_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
